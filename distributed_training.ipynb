{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjIR2NRZlSKBPsf1CnbtI4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hail-members/distributed-deep-learning/blob/main/distributed_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data parallelism"
      ],
      "metadata": {
        "id": "4wVb0Krj2l58"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVY3s5sgubw7",
        "outputId": "e2017c88-248d-4dbb-cbae-2c7057a01928"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without parallelism: Mean = 0.5001536871694922, Time = 4.7696 seconds\n",
            "With parallelism (4 threads): Mean = 0.5001536871695091, Time = 3.4092 seconds\n",
            "Speedup: 1.40x\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# 데이터를 생성 (난수)\n",
        "data = np.random.rand(10_000_000)\n",
        "\n",
        "# 점진적 평균 계산 함수 (각 스레드에서 처리)\n",
        "def calculate_mean_incremental(data_chunk):\n",
        "    mean = 0\n",
        "    for i, x in enumerate(data_chunk, start=1):\n",
        "        mean += (x - mean) / i  # 점진적 평균 업데이트\n",
        "    return mean, len(data_chunk)\n",
        "\n",
        "# 멀티 쓰레드를 사용한 병렬 점진적 평균 계산 함수\n",
        "def calculate_parallel_mean(data, num_threads=4):\n",
        "    # 데이터를 num_threads 개수만큼 나누기\n",
        "    chunk_size = len(data) // num_threads\n",
        "    chunks = [data[i * chunk_size:(i + 1) * chunk_size] for i in range(num_threads)]\n",
        "\n",
        "    # 평균 값을 병렬로 계산\n",
        "    total_sum = 0\n",
        "    total_count = 0\n",
        "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
        "        futures = {executor.submit(calculate_mean_incremental, chunk): chunk for chunk in chunks}\n",
        "\n",
        "        for future in futures:\n",
        "            chunk_mean, chunk_size = future.result()\n",
        "            total_sum += chunk_mean * chunk_size  # 각 조각의 평균을 전체 크기에 맞춰 합산\n",
        "            total_count += chunk_size\n",
        "\n",
        "    return total_sum / total_count  # 최종 평균 계산\n",
        "\n",
        "# 성능 테스트\n",
        "def performance_test():\n",
        "    # 병렬 처리 없이 점진적 평균 계산\n",
        "    start_time = time.time()\n",
        "    mean_result, _ = calculate_mean_incremental(data)\n",
        "    non_parallel_time = time.time() - start_time\n",
        "\n",
        "    # 멀티 쓰레드를 사용한 병렬 평균 계산\n",
        "    start_time = time.time()\n",
        "    parallel_mean_result = calculate_parallel_mean(data, num_threads=8)\n",
        "    parallel_time = time.time() - start_time\n",
        "\n",
        "    # 결과와 시간 비교 출력\n",
        "    print(f\"Without parallelism: Mean = {mean_result}, Time = {non_parallel_time:.4f} seconds\")\n",
        "    print(f\"With parallelism (4 threads): Mean = {parallel_mean_result}, Time = {parallel_time:.4f} seconds\")\n",
        "    print(f\"Speedup: {non_parallel_time / parallel_time:.2f}x\")\n",
        "\n",
        "# 성능 테스트 실행\n",
        "performance_test()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MLP training"
      ],
      "metadata": {
        "id": "V3XoVCI8D3fB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# MLP 모델 정의\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# MNIST 데이터 로딩\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 일반 학습 방식\n",
        "def train_simple(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]\\tLoss: {loss.item():.6f}\")\n",
        "\n",
        "# CPU에서 학습\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# 일반 학습 시간 측정\n",
        "model_simple = SimpleMLP().to(device)\n",
        "optimizer_simple = optim.SGD(model_simple.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "start_time_simple = time.time()\n",
        "train_simple(model_simple, device, train_loader, optimizer_simple, epoch=1)\n",
        "end_time_simple = time.time()\n",
        "\n",
        "simple_training_time = end_time_simple - start_time_simple\n",
        "\n",
        "\n",
        "print(f\"Simple training time: {simple_training_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKEf_NShD17j",
        "outputId": "4fb4c091-e802-45a0-ee8e-06d06944cdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000]\tLoss: 2.296316\n",
            "Train Epoch: 1 [6400/60000]\tLoss: 0.475918\n",
            "Train Epoch: 1 [12800/60000]\tLoss: 0.208591\n",
            "Train Epoch: 1 [19200/60000]\tLoss: 0.395367\n",
            "Train Epoch: 1 [25600/60000]\tLoss: 0.143054\n",
            "Train Epoch: 1 [32000/60000]\tLoss: 0.423407\n",
            "Train Epoch: 1 [38400/60000]\tLoss: 0.155348\n",
            "Train Epoch: 1 [44800/60000]\tLoss: 0.100124\n",
            "Train Epoch: 1 [51200/60000]\tLoss: 0.195753\n",
            "Train Epoch: 1 [57600/60000]\tLoss: 0.160608\n",
            "Simple training time: 19.5761 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import threading\n",
        "from queue import Queue\n",
        "import time\n",
        "\n",
        "# MLP 모델 정의\n",
        "class SimpleMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(28*28, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 28*28)  # Flatten the input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# A3C 워커 클래스 정의\n",
        "class Worker(threading.Thread):\n",
        "    def __init__(self, global_model, optimizer, train_loader, device, queue):\n",
        "        threading.Thread.__init__(self)\n",
        "        self.global_model = global_model\n",
        "        self.optimizer = optimizer\n",
        "        self.train_loader = train_loader\n",
        "        self.device = device\n",
        "        self.queue = queue\n",
        "        self.local_model = SimpleMLP().to(self.device)\n",
        "        self.local_model.load_state_dict(self.global_model.state_dict())  # 글로벌 모델 복제\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def run(self):\n",
        "        for batch_idx, (data, target) in enumerate(self.train_loader):\n",
        "            data, target = data.to(self.device), target.to(self.device)\n",
        "\n",
        "            # 로컬 모델로 학습\n",
        "            self.optimizer.zero_grad()\n",
        "            output = self.local_model(data)\n",
        "            loss = self.criterion(output, target)\n",
        "            loss.backward()\n",
        "\n",
        "            # 글로벌 모델 업데이트\n",
        "            with torch.no_grad():\n",
        "                for global_param, local_param in zip(self.global_model.parameters(), self.local_model.parameters()):\n",
        "                    global_param.grad = local_param.grad  # 글로벌 모델에 로컬 그래디언트 적용\n",
        "                self.optimizer.step()\n",
        "\n",
        "            # 로스 출력 및 글로벌 모델 저장\n",
        "            if batch_idx % 100 == 0:\n",
        "                print(f\"Worker {threading.current_thread().name} - Batch {batch_idx} Loss: {loss.item():.4f}\")\n",
        "\n",
        "# 멀티 스레드 학습\n",
        "def training(global_model, train_loader, num_workers=4):\n",
        "    optimizer = optim.SGD(global_model.parameters(), lr=0.01, momentum=0.9)\n",
        "    queue = Queue()\n",
        "    workers = []\n",
        "\n",
        "    # 각 워커 스레드 생성 및 시작\n",
        "    for i in range(num_workers):\n",
        "        worker = Worker(global_model, optimizer, train_loader, torch.device(\"cpu\"), queue)\n",
        "        worker.start()\n",
        "        workers.append(worker)\n",
        "\n",
        "    # 각 워커의 완료를 기다림\n",
        "    for worker in workers:\n",
        "        worker.join()\n",
        "\n",
        "# CPU에서 학습\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# 임의 데이터 생성 (MNIST 대신 사용)\n",
        "num_samples = 60000  # 데이터 샘플 수\n",
        "num_classes = 10  # 클래스 수\n",
        "\n",
        "X_data = torch.randn(num_samples, 1, 28, 28)  # 랜덤 이미지 데이터\n",
        "y_data = torch.randint(0, num_classes, (num_samples,))  # 랜덤 레이블 데이터\n",
        "\n",
        "# DataLoader로 변환\n",
        "train_dataset = TensorDataset(X_data, y_data)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# 글로벌 모델 생성 및 분산 학습 실행\n",
        "global_model = SimpleMLP().to(device)\n",
        "\n",
        "start_time = time.time()\n",
        "training(global_model, train_loader, num_workers=4)\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"distributed training time: {end_time - start_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iffl0I8fIAd6",
        "outputId": "d965fd2c-4c12-4968-9f13-52b054bc9cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Worker Thread-23 - Batch 0 Loss: 2.2992\n",
            "Worker Thread-24 - Batch 0 Loss: 2.3366\n",
            "Worker Thread-26 - Batch 0 Loss: 2.3325\n",
            "Worker Thread-25 - Batch 0 Loss: 2.3304\n",
            "Worker Thread-24 - Batch 100 Loss: 2.3026\n",
            "Worker Thread-23 - Batch 100 Loss: 2.2897\n",
            "Worker Thread-25 - Batch 100 Loss: 2.2978\n",
            "Worker Thread-26 - Batch 100 Loss: 2.3156\n",
            "Worker Thread-24 - Batch 200 Loss: 2.3023\n",
            "Worker Thread-25 - Batch 200 Loss: 2.3279\n",
            "Worker Thread-23 - Batch 200 Loss: 2.2960\n",
            "Worker Thread-26 - Batch 200 Loss: 2.3019\n",
            "Worker Thread-24 - Batch 300 Loss: 2.2990\n",
            "Worker Thread-25 - Batch 300 Loss: 2.3066\n",
            "Worker Thread-26 - Batch 300 Loss: 2.3111\n",
            "Worker Thread-23 - Batch 300 Loss: 2.3196\n",
            "Worker Thread-24 - Batch 400 Loss: 2.3292\n",
            "Worker Thread-25 - Batch 400 Loss: 2.3094\n",
            "Worker Thread-23 - Batch 400 Loss: 2.3149\n",
            "Worker Thread-26 - Batch 400 Loss: 2.3131\n",
            "Worker Thread-24 - Batch 500 Loss: 2.3117\n",
            "Worker Thread-25 - Batch 500 Loss: 2.3063\n",
            "Worker Thread-23 - Batch 500 Loss: 2.3036\n",
            "Worker Thread-26 - Batch 500 Loss: 2.3187\n",
            "Worker Thread-24 - Batch 600 Loss: 2.3284\n",
            "Worker Thread-25 - Batch 600 Loss: 2.2988\n",
            "Worker Thread-23 - Batch 600 Loss: 2.3044\n",
            "Worker Thread-26 - Batch 600 Loss: 2.3075\n",
            "Worker Thread-24 - Batch 700 Loss: 2.3062\n",
            "Worker Thread-25 - Batch 700 Loss: 2.3033\n",
            "Worker Thread-23 - Batch 700 Loss: 2.2971\n",
            "Worker Thread-26 - Batch 700 Loss: 2.2984\n",
            "Worker Thread-24 - Batch 800 Loss: 2.2843\n",
            "Worker Thread-25 - Batch 800 Loss: 2.2961\n",
            "Worker Thread-26 - Batch 800 Loss: 2.3095\n",
            "Worker Thread-23 - Batch 800 Loss: 2.3260\n",
            "Worker Thread-24 - Batch 900 Loss: 2.2923\n",
            "Worker Thread-25 - Batch 900 Loss: 2.3220\n",
            "Worker Thread-23 - Batch 900 Loss: 2.3172\n",
            "Worker Thread-26 - Batch 900 Loss: 2.3152\n",
            "distributed training time: 14.1005 seconds\n"
          ]
        }
      ]
    }
  ]
}