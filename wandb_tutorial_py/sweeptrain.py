import wandb
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader

# --- 1. ëª¨ë¸ ì •ì˜ ---
# 'hidden_size'ë¥¼ wandb.configì—ì„œ ë°›ì•„ì˜µë‹ˆë‹¤.
class SimpleMLP(nn.Module):
    def __init__(self, input_size=784, hidden_size=64, output_size=10):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # (N, 1, 28, 28) -> (N, 784)
        x = x.view(x.shape[0], -1) 
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        # CrossEntropyLossëŠ” softmaxë¥¼ í¬í•¨í•˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” log_softmax ë¶ˆí•„ìš”
        return x

# --- 2. ë°ì´í„° ë¡œë” ì¤€ë¹„ ---
def get_data_loaders(batch_size=64):
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.1307,), (0.3081,))
    ])
    
    train_dataset = torchvision.datasets.MNIST(
        root='./data', train=True, download=True, transform=transform
    )
    test_dataset = torchvision.datasets.MNIST(
        root='./data', train=False, download=True, transform=transform
    )
    
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)
    
    return train_loader, test_loader

# --- 3. í•™ìŠµ ë° í‰ê°€ ë¡œì§ ---
def train_and_evaluate():
    # â­ (ì¤‘ìš”) wandb.init() í˜¸ì¶œ
    # Sweep Controllerê°€ ì´ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•  ë•Œ config ê°’ì„ ì£¼ì…í•©ë‹ˆë‹¤.
    run = wandb.init(
        name = "mlp-sweep-run",  # ê° ì‹¤í–‰(run)ì˜ ì´ë¦„
        )
    
    # â­ (ì¤‘ìš”) Sweepì—ì„œ ì •ì˜í•œ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì ‘ê·¼
    # configì˜ ê¸°ë³¸ê°’(defaults)ì„ ì„¤ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
    config = wandb.config
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # 1. ë°ì´í„° ë¡œë“œ
    train_loader, test_loader = get_data_loaders()
    
    # 2. ëª¨ë¸ ìƒì„± (config ê°’ ì‚¬ìš©)
    model = SimpleMLP(
        hidden_size=config.hidden_size
    ).to(device)
    
    # 3. ì˜µí‹°ë§ˆì´ì € ìƒì„± (config ê°’ ì‚¬ìš©)
    optimizer = optim.Adam(
        model.parameters(), 
        lr=config.learning_rate
    )
    
    criterion = nn.CrossEntropyLoss()
    
    # 4. í•™ìŠµ ë£¨í”„ (config ê°’ ì‚¬ìš©)
    for epoch in range(config.epochs):
        # Train
        model.train()
        for data, target in train_loader:
            data, target = data.to(device), target.to(device)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
        
        # Test (ë§¤ ì—í¬í¬ë§ˆë‹¤)
        model.eval()
        test_loss = 0
        correct = 0
        with torch.no_grad():
            for data, target in test_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                test_loss += criterion(output, target).item() * data.size(0) # ë°°ì¹˜ ë¡œìŠ¤ í•©
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()

        test_loss /= len(test_loader.dataset)
        test_accuracy = 100. * correct / len(test_loader.dataset)
        
        print(f"Epoch {epoch}: Test Loss={test_loss:.4f}, Acc={test_accuracy:.2f}%")
        
        # ğŸ“ˆ (ì¤‘ìš”) wandbì— Metric ë¡œê¹…
        # Sweepì€ ì´ ê°’('test_loss')ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœì í™”ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        wandb.log({
            "epoch": epoch,
            "Test/test loss": test_loss,
            "test_accuracy": test_accuracy
        })

if __name__ == "__main__":
    train_and_evaluate()