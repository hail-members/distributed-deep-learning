{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53385e69",
   "metadata": {},
   "source": [
    "## 설치\n",
    "\n",
    "deepspeed 는 자체적인 버전과 transformers 와의 통합을 위한 버전을 모두 제공합니다. 물론 파이토치와의 호환성도 당얀히 있습니다.\n",
    "\n",
    "여기서는 일단 딥스피드를 설치하고, 딥스피드를 이용해 torch 모델, 그리고 간단한 transformers 모델을 실행하는 방법을 설명합니다.\n",
    "\n",
    "예제들은 공식 DeepspeedExample 레포지토리에서 발췌했습니다! https://github.com/deepspeedai/DeepSpeedExamples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1268fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting deepspeed\n",
      "  Downloading deepspeed-0.18.2.tar.gz (1.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: transformers[deepspeed] in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (4.57.1)\n",
      "Collecting einops (from deepspeed)\n",
      "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting hjson (from deepspeed)\n",
      "  Using cached hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting msgpack (from deepspeed)\n",
      "  Downloading msgpack-1.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting ninja (from deepspeed)\n",
      "  Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from deepspeed) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from deepspeed) (25.0)\n",
      "Requirement already satisfied: psutil in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from deepspeed) (7.1.0)\n",
      "Collecting py-cpuinfo (from deepspeed)\n",
      "  Using cached py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: pydantic>=2.0.0 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from deepspeed) (2.12.2)\n",
      "Requirement already satisfied: torch in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from deepspeed) (2.8.0)\n",
      "Requirement already satisfied: tqdm in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from deepspeed) (4.67.1)\n",
      "Collecting nvidia-ml-py (from deepspeed)\n",
      "  Using cached nvidia_ml_py-13.580.82-py3-none-any.whl.metadata (9.6 kB)\n",
      "Requirement already satisfied: filelock in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from transformers[deepspeed]) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from transformers[deepspeed]) (0.35.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from transformers[deepspeed]) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from transformers[deepspeed]) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from transformers[deepspeed]) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from transformers[deepspeed]) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from transformers[deepspeed]) (0.6.2)\n",
      "Collecting accelerate>=0.26.0 (from transformers[deepspeed])\n",
      "  Using cached accelerate-1.11.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[deepspeed]) (2025.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[deepspeed]) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers[deepspeed]) (1.1.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed) (2.41.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from pydantic>=2.0.0->deepspeed) (0.4.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from torch->deepspeed) (3.4.0)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from triton==3.4.0->torch->deepspeed) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from sympy>=1.13.3->torch->deepspeed) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from jinja2->torch->deepspeed) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from requests->transformers[deepspeed]) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from requests->transformers[deepspeed]) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from requests->transformers[deepspeed]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hail/anaconda3/envs/dist/lib/python3.11/site-packages (from requests->transformers[deepspeed]) (2025.10.5)\n",
      "Using cached accelerate-1.11.0-py3-none-any.whl (375 kB)\n",
      "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
      "Using cached hjson-3.1.0-py3-none-any.whl (54 kB)\n",
      "Downloading msgpack-1.1.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (426 kB)\n",
      "Using cached ninja-1.13.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (180 kB)\n",
      "Using cached nvidia_ml_py-13.580.82-py3-none-any.whl (49 kB)\n",
      "Using cached py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: deepspeed\n",
      "\u001b[33m  DEPRECATION: Building 'deepspeed' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'deepspeed'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for deepspeed (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for deepspeed: filename=deepspeed-0.18.2-py3-none-any.whl size=1763448 sha256=c4dbd797028c2018f77b77c35f3023172ed6381a9c24d948ea7395b264b5baa0\n",
      "  Stored in directory: /home/hail/.cache/pip/wheels/80/2b/fe/f2625302f25976b9828a27d3d2567bdc1d587a1ff0ab42c8a9\n",
      "Successfully built deepspeed\n",
      "Installing collected packages: py-cpuinfo, nvidia-ml-py, hjson, ninja, msgpack, einops, deepspeed, accelerate\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8/8\u001b[0m [accelerate]8\u001b[0m [accelerate]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-1.11.0 deepspeed-0.18.2 einops-0.8.1 hjson-3.1.0 msgpack-1.1.2 ninja-1.13.0 nvidia-ml-py-13.580.82 py-cpuinfo-9.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install deepspeed transformers[deepspeed]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486bc2f",
   "metadata": {},
   "source": [
    "## 1. 간단한 deepspeed 설정해서 돌려보기\n",
    "\n",
    "딥스피드의 설정은 json 파일로 만들어 넣어주게 됩니다. `ds_config.json` 파일을 만들어서 저장합니다.\n",
    "```json\n",
    "{\n",
    "  \"fp16\": {\n",
    "    \"enabled\": true,\n",
    "    \"loss_scale\": 0,\n",
    "    \"loss_scale_window\": 1000,\n",
    "    \"initial_scale_power\": 16,\n",
    "    \"hysteresis\": 2,\n",
    "    \"min_loss_scale\": 1\n",
    "  },\n",
    "  \"optimizer\": {\n",
    "    \"type\": \"AdamW\",\n",
    "    \"params\": {\n",
    "      \"lr\": 0.001,\n",
    "      \"betas\": [0.9, 0.999],\n",
    "      \"eps\": 1e-8,\n",
    "      \"weight_decay\": 3e-7\n",
    "    }\n",
    "  },\n",
    "  \"scheduler\": {\n",
    "    \"type\": \"WarmupLR\",\n",
    "    \"params\": {\n",
    "      \"warmup_min_lr\": 0,\n",
    "      \"warmup_max_lr\": 0.001,\n",
    "      \"warmup_num_steps\": 500\n",
    "    }\n",
    "  },\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,\n",
    "    \"allgather_partitions\": true,\n",
    "    \"allgather_bucket_size\": 2e8,\n",
    "    \"reduce_scatter\": true,\n",
    "    \"reduce_bucket_size\": 2e8,\n",
    "    \"contiguous_gradients\": true,\n",
    "    \"overlap_comm\": true\n",
    "  },\n",
    "  \"train_micro_batch_size_per_gpu\": 10\n",
    "}\n",
    "```\n",
    "\n",
    "각각이 의미하는 바는 다음과 같습니다.\n",
    "`\"fp16\"` : 16비트 부동소수점 연산을 사용할지 여부와 관련된 설정입니다.  \n",
    "`\"optimizer\"` : 옵티마이저의 종류와 하이퍼파라미터를 설정합니다.  \n",
    "`\"scheduler\"` : 학습률 스케줄러의 종류와 하이퍼파라미터를 설정합니다.  \n",
    "`\"zero_optimization\"` : ZeRO 최적화 기법의 단계를 설정합니다. 여기서 `\"stage\": 2` 는 ZeRO-2 를 사용하는 것을 의미합니다. \n",
    "`\"train_micro_batch_size_per_gpu\"` : 각 GPU당 마이크로 배치 크기를 설정합니다.\n",
    "\n",
    "주의할 점은 여기서 **optimizer** 설정이 pytorch 의 설정으로 이루어지거나, transformers의 train 메서드에서 설정하는 것과 다르다는 점입니다. 별도로 deepspeed 설정에서 옵티마이저를 지정해주어야 합니다.\n",
    "\n",
    "더 다양한 config 설정하는 법을 보고 싶다면 https://www.deepspeed.ai/docs/config-json/ 를 참고해볼 수 있습니다. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4322710",
   "metadata": {},
   "source": [
    "### 1.1. 토치에서 쓰기: `cifar/cifar10_deepspeed.py`\n",
    "\n",
    "1. deepspeed를 import 합니다.\n",
    "2. argument 에서 ds_config에서 세세하게 지정하지 않는 것들을 설정해줍니다. (예: epoch, batch size, 혹은 그 외 네트워크 설정이나 실험에 필요한거)\n",
    "3. `deepspeed.add_config_arguments(parser)` 로 deepspeed 모델 엔진을 위한 인자를 추가합니다. 여기서 아까 작성해놓은 `ds_config.json` 파일을 지정해줄 수 있습니다.\n",
    "4. 모델 초기화 시 `deepspeed.initialize()` 를 사용합니다. 여기서 모델을 지정해주고 그것에 대한 `model_engine` 인스턴스를 받습니다. 이거를 통해서 학습 루프를 돌리게 됩니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc925113",
   "metadata": {},
   "source": [
    "### 가끔 kill 을 안하고 종료하는경우\n",
    "\n",
    "포트가 이미 사용중이라는 에러가 뜨는 경우가 있습니다.\n",
    "\n",
    "```\n",
    "torch.distributed.DistNetworkError: The server socket has failed to listen on any local network address. port: 29500, useIpv6: false, code: -98, name: EADDRINUSE, message: address already in use\n",
    "```\n",
    "\n",
    "이렇게 해당 포트번호로 사용중인 프로세스를 확인합니다.\n",
    "\n",
    "```\n",
    "(dist) hail@hail-4090-3way:~/HDD/dk/distributed-deep-learning/deepspeed/cifar$ lsof -i :29500\n",
    "COMMAND       PID USER   FD   TYPE     DEVICE SIZE/OFF NODE NAME\n",
    "pt_data_w 1629447 hail   41u  IPv6 1423925726      0t0  TCP localhost:49956->localhost:29500 (ESTABLISHED)\n",
    "pt_data_w 1629448 hail   47u  IPv6 1423964615      0t0  TCP *:29500 (LISTEN)\n",
    "pt_data_w 1629448 hail   48u  IPv6 1423964616      0t0  TCP localhost:43806->localhost:29500 (ESTABLISHED)\n",
    "pt_data_w 1629448 hail   49u  IPv6 1423655935      0t0  TCP localhost:29500->localhost:43806 (ESTABLISHED)\n",
    "pt_data_w 1629448 hail   50u  IPv6 1423927837      0t0  TCP localhost:29500->localhost:49956 (ESTABLISHED)\n",
    "pt_data_w 1629449 hail   41u  IPv6 1423925726      0t0  TCP localhost:49956->localhost:29500 (ESTABLISHED)\n",
    "pt_data_w 1629451 hail   47u  IPv6 1423964615      0t0  TCP *:29500 (LISTEN)\n",
    "pt_data_w 1629451 hail   48u  IPv6 1423964616      0t0  TCP localhost:43806->localhost:29500 (ESTABLISHED)\n",
    "pt_data_w 1629451 hail   49u  IPv6 1423655935      0t0  TCP localhost:29500->localhost:43806 (ESTABLISHED)\n",
    "pt_data_w 1629451 hail   50u  IPv6 1423927837      0t0  TCP localhost:29500->localhost:49956 (ESTABLISHED)\n",
    "```\n",
    "\n",
    "찾아서 PID 제거\n",
    "\n",
    "```\n",
    "(dist) hail@hail-4090-3way:~/HDD/dk/distributed-deep-learning/deepspeed/cifar$ kill -9 1629448\n",
    "(dist) hail@hail-4090-3way:~/HDD/dk/distributed-deep-learning/deepspeed/cifar$ kill -9 1629449\n",
    "(dist) hail@hail-4090-3way:~/HDD/dk/distributed-deep-learning/deepspeed/cifar$ kill -9 1629451\n",
    "(dist) hail@hail-4090-3way:~/HDD/dk/distributed-deep-learning/deepspeed/cifar$ kill -9 1629447\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58038fba",
   "metadata": {},
   "source": [
    "## 2. Pipeline Parallelism\n",
    "\n",
    "![https://www.deepspeed.ai/assets/images/pipe-schedule.png](https://www.deepspeed.ai/assets/images/pipe-schedule.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee04d01a",
   "metadata": {},
   "source": [
    "DeepSpeed는 **그래디언트 누적(Gradient Accumulation)** 을 사용하여 파이프라인 병렬처리를 구현합니다.\n",
    "\n",
    "- 하나의 훈련 배치는 여러 개의 **마이크로배치(micro-batch)** 로 분할됩니다.\n",
    "- 이 마이크로배치들은 파이프라인 스테이지에서 병렬로 처리됩니다.\n",
    "- 한 스테이지가 마이크로배치에 대한 Forward Pass(순전파)를 완료하면, 그 결과물(Activation)은 즉시 파이프라인의 다음 스테이지로 전달됩니다.\n",
    "- 마찬가지로, 다음 스테이지가 Backward Pass(역전파)를 완료하면, 그래디언트가 이전 스테이지로 거꾸로 전달됩니다.\n",
    "- 각 스테이지는 Backward Pass를 수행하며 그래디언트를 로컬에 누적합니다. \n",
    "- (파이프라인이 한 번 다 돈 뒤) 모든 데이터 병렬 그룹(DP group)은 동시에 그래디언트 All-Reduce를 수행합니다.\n",
    "- 마지막으로 옵티마이저가 모델 가중치를 업데이트합니다.\n",
    "\n",
    "위 그림은 8개의 마이크로배치를 사용하는 하이브리드(2-way DP + 2-stage PP) 훈련 예시입니다.\n",
    "- GPU 0과 GPU 2는 파이프라인(PP)으로 묶여 Forward(F)와 Backward(B)를 번갈아 수행합니다.\n",
    "- 그 후, 이들은 각자의 데이터 병렬(DP) 파트너인 GPU 1, GPU 3과 각각 All-Reduce(AR)를 수행합니다.\n",
    "- 최종적으로 두 파이프라인 스테이지가 각자의 가중치를 업데이트(U)합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee05aa2",
   "metadata": {},
   "source": [
    "파이프라인 병렬처리를 사용하려면 모델이 **레이어의 순차적인 나열(sequence of layers)** 로 표현되어야 합니다. PyTorch의 torch.nn.Sequential이 파이프라인 병렬화에 가장 적절하며, DeepSpeed는 이를 코드 수정 없이 바로 병렬화할 수 있습니다.\n",
    "\n",
    "DeepSpeed는 PipelineModule이라는 래퍼(wrapper)를 제공합니다.\n",
    "\n",
    "```python\n",
    "# 기존 nn.Sequential 모델\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(in_features, hidden_dim),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.Linear(hidden_dim, out_features)\n",
    ")\n",
    "\n",
    "# DeepSpeed PipelineModule로 변환\n",
    "from deepspeed.pipe import PipelineModule\n",
    "net = PipelineModule(layers=net, num_stages=2)\n",
    "```\n",
    "\n",
    "PipelineModule은 layers 인자로 받은 모델을 num_stages=2 (즉, 2개의 스테이지)로 분할하여 각각의 GPU에 할당합니다. 만약 2개보다 많은 GPU가 있다면, DeepSpeed는 자동으로 하이브리드 데이터 병렬처리(DP)도 함께 사용합니다.\n",
    "\n",
    "참고: 총 GPU 개수는 파이프라인 스테이지 개수로 나누어 떨어져야 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aeb671",
   "metadata": {},
   "source": [
    "### (예시) AlexNet을 파이프라인 모델로 변환하기\n",
    "\n",
    "AlexNet은 여러 nn.Sequential 서브 모듈로 구성되어 있습니다. 이를 PipelineModule로 만들려면, 이 서브 모듈들을 하나의 평평한(flat) 레이어 시퀀스로 만들어 반환하는 헬퍼(helper) 함수를 추가하면 됩니다.\n",
    "\n",
    "```python\n",
    "# 기존 AlexNet 정의\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(...)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(...)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# DeepSpeed 파이프라인용으로 수정\n",
    "class AlexNetPipe(AlexNet):\n",
    "    def to_layers(self):\n",
    "        # 모든 서브 모듈을 하나의 리스트로 펼칩니다.\n",
    "        layers = [\n",
    "            *self.features,\n",
    "            self.avgpool,\n",
    "            lambda x: torch.flatten(x, 1), # 일반 함수(lambda)도 레이어가 될 수 있습니다.\n",
    "            *self.classifier\n",
    "        ]\n",
    "        return layers\n",
    "\n",
    "# PipelineModule로 최종 변환\n",
    "from deepspeed.pipe import PipelineModule\n",
    "net = AlexNetPipe()\n",
    "net = PipelineModule(layers=net.to_layers(), num_stages=2)\n",
    "```\n",
    "\n",
    "### 훈련 루프 (Training Loops)\n",
    "\n",
    "파이프라인 병렬처리는 Forward와 Backward Pass를 겹쳐서(interleave) 실행합니다. 따라서 기존처럼 forward(), backward(), step()을 명시적으로 분리하여 호출할 수 없습니다.\n",
    "\n",
    "대신, DeepSpeed 엔진은 train_batch()라는 단일 메서드를 제공합니다. 이 메서드는 다음 배치의 데이터가 소모되고 모델 가중치가 업데이트될 때까지 파이프라인을 알아서 실행합니다.\n",
    "\n",
    "```python\n",
    "# 기존 Data Parallel 훈련 루프 (참고용)\n",
    "data_iter = iter(train_loader)\n",
    "for micro_batch in engine.gradient_accumulation_steps():\n",
    "    batch = next(data_iter)\n",
    "    loss = engine(batch)\n",
    "    engine.backward(loss)\n",
    "engine.step()\n",
    "\n",
    "# DeepSpeed Pipeline 훈련 루프 (이것만 쓰면 됨)\n",
    "train_iter = iter(train_loader)\n",
    "loss = engine.train_batch(data_iter=train_iter)\n",
    "```\n",
    "\n",
    "### 데이터 처리 (Dealing with Data)\n",
    "\n",
    "파이프라인 환경에서는 첫 번째 스테이지(Stage 0)만 입력 데이터를 사용하고, 마지막 스테이지만 라벨(label)을 사용해 손실(loss)을 계산합니다.\n",
    "\n",
    "DeepSpeed 엔진은 데이터 로더가 (입력 데이터, 라벨 데이터) 형태의 튜플을 반환할 것으로 기대합니다.\n",
    "\n",
    "가장 편리한 방법은 deepspeed.initialize()에 training_data 인자로 dataset을 넘겨주는 것입니다. DeepSpeed가 알아서 분산 데이터 로더를 구성해 줍니다. 이 경우 훈련 루프는 훨씬 더 단순해집니다.\n",
    "\n",
    "```python\n",
    "# dataset을 initialize에 전달\n",
    "engine, _, _, _ = deepspeed.initialize(\n",
    "    args=args,\n",
    "    model=net,\n",
    "    model_parameters=[...],\n",
    "    training_data=cifar_trainset() # 데이터셋을 여기서 전달\n",
    ")\n",
    "\n",
    "# 훈련 루프\n",
    "for step in range(args.steps):\n",
    "    loss = engine.train_batch() # 데이터 이터레이터를 넘길 필요도 없음\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d9356a",
   "metadata": {},
   "source": [
    "## 3. Automatic tensor paralleism (transformers) \n",
    "\n",
    "### 서론\n",
    "\n",
    "이 튜토리얼은 \\*\\*추론(Inference)\\*\\*을 위한 새로운 **자동 텐서 병렬처리(Automatic Tensor Parallelism)** 기능을 소개합니다.\n",
    "\n",
    "이전에는 텐서 병렬처리(TP)를 활성화하기 위해 사용자가 DeepSpeed에 \\*\\*'주입 정책(injection policy)'\\*\\*을 직접 제공해야 했습니다. 하지만 이제 DeepSpeed는 커널 주입(kernel injection)이 활성화되지 않고 주입 정책이 제공되지 않은 경우, **HuggingFace 모델에 대해 기본적으로 자동 텐서 병렬처리를 지원**합니다.\n",
    "\n",
    "이 기능 덕분에, 현재 커널 주입이 지원되지 않는 모델이라도 사용자가 직접 주입 정책을 제공할 필요 없이 성능을 향상시킬 수 있습니다.\n",
    "\n",
    "**커널 주입이란?** 원래 GPU에서 실행되는 특정 연산(예: 행렬 곱셈)을 최적화된 연산들로 대체하는 것을 의미합니다.\n",
    "예를 들어... GPU0과 GPU1 이 있을때 VRAM으로 올리고 - GPU0 연산하고, GPU1 연산하고 - 결과를 합치는 식으로 최적화하던 것을 tensor parallelism 에 맞게 일부는 GPU0 에서, 일부는 GPU1 에서 동시에 연산하고 합치고 하는 식으로 직접 선언해서 최적화하는 것을 뜻합니다.\n",
    "\n",
    "\n",
    "```python\n",
    "# ---------------------------------------\n",
    "# 새로운 자동 텐서 병렬처리 방식\n",
    "# ---------------------------------------\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import deepspeed\n",
    "\n",
    "local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n",
    "\n",
    "# 모델 파이프라인 생성\n",
    "pipe = transformers.pipeline(task=\"text2text-generation\", model=\"google/t5-v1_1-small\", device=local_rank)\n",
    "\n",
    "# DeepSpeed-Inference 엔진 초기화\n",
    "# ★★★ tp_size (mp_size)만 지정하면 자동으로 TP가 적용됩니다 ★★★\n",
    "pipe.model = deepspeed.init_inference(\n",
    "    pipe.model,\n",
    "    mp_size=world_size, # mp_size는 tp_size와 같습니다.\n",
    "    dtype=torch.float\n",
    ")\n",
    "\n",
    "output = pipe('Input String')\n",
    "```\n",
    "\n",
    "이전에는 커널 주입이 지원되지 않는 모델에 텐서 병렬처리만 적용하려면, Transformer의 Encoder/Decoder 레이어에 있는 특정 두 Linear 레이어(1. 어텐션 출력 GeMM, 2. 레이어 출력 GeMM)를 명시하는 주입 정책을 전달해야 했습니다. 이는 GPU 간의 All-Reduce 통신을 추가하여 부분적인 결과를 병합하기 위해 필요했습니다.\n",
    "\n",
    "아래는 **이전 방식**의 예시입니다.\n",
    "\n",
    "```python\n",
    "# ----------------------------------\n",
    "# 이전의 텐서 병렬처리 방식\n",
    "# ----------------------------------\n",
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import deepspeed\n",
    "from transformers.models.t5.modeling_t5 import T5Block # ★모델 내부 블록 임포트 필요\n",
    "\n",
    "local_rank = int(os.getenv(\"LOCAL_RANK\", \"0\"))\n",
    "world_size = int(os.getenv(\"WORLD_SIZE\", \"1\"))\n",
    "\n",
    "# 모델 파이프라인 생성\n",
    "pipe = transformers.pipeline(task=\"text2text-generation\", model=\"google/t5-v1_1-small\", device=local_rank)\n",
    "\n",
    "# DeepSpeed-Inference 엔진 초기화\n",
    "# ★★★ injection_policy를 수동으로 지정해야 했음 ★★★\n",
    "pipe.model = deepspeed.init_inference(\n",
    "    pipe.model,\n",
    "    mp_size=world_size,\n",
    "    dtype=torch.float,\n",
    "    injection_policy={T5Block: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')}\n",
    ")\n",
    "output = pipe('Input String')\n",
    "```\n",
    "\n",
    "여기서 `injection_policy={T5Block: ('SelfAttention.o', 'EncDecAttention.o', 'DenseReluDense.wo')}` 가 각각 T5Block 내의 Self-Attention 출력, Encoder-Decoder Attention 출력, Feed-Forward 네트워크 출력에 해당하는 Linear 레이어들을 지정합니다. 이 레이어들에 텐서 병렬처리가 적용되며, DeepSpeed는 자동으로 필요한 All-Reduce 연산을 삽입합니다. 즉 여기 선언된 연산들에 들어가는 Tensor들을 GPU로 분할하고, 동시에 연산하고, 합치는 방식의 최적화를 선언합니다.\n",
    "\n",
    "\n",
    "자동 텐서 병렬처리를 사용하면, 지원되는 모델에 대해 더 이상 주입 정책을 제공할 필요가 없습니다. 주입 정책은 런타임에 결정되어 자동으로 적용됩니다.\n",
    "\n",
    "-----\n",
    "\n",
    "### 예제 스크립트\n",
    "\n",
    "https://github.com/deepspeedai/DeepSpeedExamples/blob/master/inference/huggingface/text-generation/inference-test.py\n",
    "\n",
    "Inference 테스트 스크립트(`inference-test.py`)를 사용하여 자동 텐서 병렬처리의 성능 향상을 관찰할 수 있습니다. 이 스크립트는 텍스트 생성 모델을 테스트하기 위한 것이며, 비교를 위해 토큰당 지연 시간(latency), 대역폭(bandwidth), 처리량(throughput), 메모리 사용량을 확인하는 기능이 포함되어 있습니다.\n",
    "\n",
    "#### 스크립트 실행\n",
    "\n",
    "다음 명령어는 DeepSpeed나 텐서 병렬처리 없이 모델을 실행합니다. (`test_performance` 플래그로 성능 데이터를 수집합니다.)\n",
    "\n",
    "```bash\n",
    "deepspeed --num_gpus <num_gpus> DeepSpeedExamples/inference/huggingface/text-generation/inference-test.py \\\n",
    "    --model <model> \\\n",
    "    --batch_size <batch_size> \\\n",
    "    --test_performance\n",
    "```\n",
    "\n",
    "텐서 병렬처리를 활성화하려면, 호환되는 모델에 대해 `ds_inference` 플래그를 사용하면 됩니다. (이 플래그가 자동 TP를 활성화합니다.)\n",
    "\n",
    "```bash\n",
    "deepspeed --num_gpus <num_gpus> DeepSpeedExamples/inference/huggingface/text-generation/inference-test.py \\\n",
    "    --model <model> \\\n",
    "    --batch_size <batch_size> \\\n",
    "    --test_performance \\\n",
    "    --ds_inference\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 우리 예제\n",
    "\n",
    "```terminal\n",
    "# 해당디렉토리로 이동해서 실행\n",
    "deepspeed --num_gpus 2 inference-test.py --model facebook/opt-125m\n",
    "# 비교용\n",
    "deepspeed --num_gpus 1 inference-test.py --model facebook/opt-125m\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "(albert, arctic, baichuan, bert, bloom, codellama, falcon, gpt-j, gpt-neo, gpt-neox, llama, llama2, mistral, mixtral, mpt, opt, phi, qwen, qwen2, t5, xglm 등 다수)\n",
    "\n",
    "### 지원되지 않는 모델\n",
    "\n",
    "다음 모델은 현재 자동 텐서 병렬처리를 지원하지 않습니다. (하지만 Bloom의 커널 주입과 같은 다른 DeepSpeed 기능과는 여전히 호환될 수 있습니다.)\n",
    "\n",
    "(deberta, flaubert, fsmt, gpt2, led, longformer, xlm, xlnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d9be0d",
   "metadata": {},
   "source": [
    "## +) ZeRO-offload ZeRO-infinity\n",
    "\n",
    "GPU 나눠서하는거 좋다.. 근데 VRAM이 여전히 작아버릴 수도 있잖아요? 그때 RAM과 NVMe(대충 빠르다는 뜻) SSD까지 사용합니다.\n",
    "\n",
    "### 1\\. The Problem: 왜 ZeRO-3로도 부족한가?\n",
    "\n",
    "ZeRO-3는 모델 가중치(Parameters)를 모든 GPU에 쪼개서(shard) 저장합니다. 하지만 여전히 **3가지**의 큰 메모리 소비 요소가 GPU VRAM을 차지합니다.\n",
    "\n",
    "1.  **모델 가중치 (Parameters):** 1T 모델이면 1T \\* 2(fp16) = 2TB. 8개 GPU로 쪼개도 GPU당 256GB. (불가능)\n",
    "2.  **그래디언트 (Gradients):** 훈련 중 발생하는 기울기 (가중치와 크기 동일)\n",
    "3.  **옵티마이저 상태 (Optimizer States):** Adam 옵티마이저의 경우, 가중치의 **12배\\~16배** 크기. (가장 큰 범인)\n",
    "\n",
    "ZeRO-3가 아무리 잘게 쪼개도, GPU VRAM(예: 80GB)은 이들을 감당할 수 없습니다.\n",
    "\n",
    "### 2\\. The Solution: ZeRO-Offload (CPU로의 위임)\n",
    "\n",
    "ZeRO-Offload의 핵심 아이디어는 간단합니다.\n",
    "\n",
    "> \"GPU는 **계산**에만 집중하고, **저장**은 VRAM보다 훨씬 용량이 큰 **CPU RAM**에 맡기자.\"\n",
    "\n",
    "GPU는 당장 계산에 필요한 데이터만 CPU RAM에서 가져오고, 계산이 끝나면 다시 CPU RAM으로 돌려보냅니다.\n",
    "\n",
    "  * **무엇을 Offload 하는가?**\n",
    "      * **옵티마이저 상태 (`offload_optimizer`):** 가장 크고, `optimizer.step()` 순간에만 잠깐 필요합니다. Offload 1순위이며 효과가 가장 좋습니다. (ZeRO-2와도 호환됩니다)\n",
    "      * **모델 가중치 (`offload_param`):** ZeRO-3와 함께 사용되며, 모델 가중치 자체도 CPU RAM에 둡니다.\n",
    "\n",
    "-----\n",
    "\n",
    "### 3\\. `ds_config.json` 예시\n",
    "\n",
    "강의에서 이 JSON 설정의 변화를 보여주는 것이 핵심입니다.\n",
    "\n",
    "#### 예시 1: ZeRO-2 + 옵티마이저 Offload (가장 실용적인 조합)\n",
    "\n",
    "VRAM은 부족하지만 CPU RAM은 넉넉할 때 최고의 선택입니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 2,\n",
    "    \n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": true\n",
    "    }\n",
    "  },\n",
    "  \"train_micro_batch_size_per_gpu\": 16\n",
    "}\n",
    "```\n",
    "\n",
    "  * `\"device\": \"cpu\"`: 옵티마이저 상태(Adam의 모멘텀 등)를 VRAM이 아닌 CPU RAM에 저장합니다.\n",
    "\n",
    "#### 예시 2: ZeRO-3 + 파라미터 & 옵티마이저 Offload (초거대 모델)\n",
    "\n",
    "모델 자체가 너무 커서 VRAM에 쪼개서도 못 올릴 때 사용합니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \n",
    "    \"offload_param\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": true\n",
    "    },\n",
    "\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"cpu\",\n",
    "      \"pin_memory\": true\n",
    "    }\n",
    "  },\n",
    "  \"train_micro_batch_size_per_gpu\": 16\n",
    "}\n",
    "```\n",
    "\n",
    "  * `\"offload_param\"`: ZeRO-3로 쪼갠 모델 가중치마저 CPU RAM에 둡니다.\n",
    "\n",
    "-----\n",
    "\n",
    "### 4\\. ZeRO-Infinity (NVMe SSD 활용)\n",
    "\n",
    "**\"CPU RAM (예: 512GB)으로도 감당이 안 되는 수십 TB급 모델은 어떡할까요?\"**\n",
    "\n",
    "이것이 **ZeRO-Infinity**가 해결하는 문제입니다.\n",
    "\n",
    "ZeRO-Infinity는 메모리 계층을 VRAM, CPU RAM에서 한 단계 더 확장하여, 용량이 TB 단위인 **NVMe SSD**까지 저장소로 활용합니다.\n",
    "\n",
    "  * **동작 원리:** 3-Tier 메모리 관리\n",
    "\n",
    "    1.  **VRAM (GPU):** 현재 계산에 필요한 데이터만 위치 (가장 빠름)\n",
    "    2.  **CPU RAM:** VRAM에서 밀려난 데이터 + 곧 필요할 데이터 (중간)\n",
    "    3.  **NVMe SSD:** 당장 필요 없는 거대한 모델/옵티마이저 상태 (가장 큼, 가장 느림)\n",
    "\n",
    "  * DeepSpeed는 '사전 페치(pre-fetching)' 기술을 사용해, GPU가 특정 레이어를 계산하기 *직전에* NVMe -\\> CPU RAM -\\> VRAM으로 데이터를 미리 불러와서 느린 속도를 최대한 숨깁니다.\n",
    "\n",
    "#### `ds_config.json` (NVMe 활성화 예시)\n",
    "\n",
    "`\"device\"`를 `\"cpu\"` 대신 `\"nvme\"`로 바꿉니다.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"zero_optimization\": {\n",
    "    \"stage\": 3,\n",
    "    \n",
    "    \"offload_param\": {\n",
    "      \"device\": \"nvme\",\n",
    "      \"pin_memory\": true,\n",
    "      \"nvme_path\": \"/path/to/my/fast_nvme_storage\" \n",
    "    },\n",
    "\n",
    "    \"offload_optimizer\": {\n",
    "      \"device\": \"nvme\",\n",
    "      \"pin_memory\": true,\n",
    "      \"nvme_path\": \"/path/to/my/fast_nvme_storage\"\n",
    "    }\n",
    "  },\n",
    "  \"train_micro_batch_size_per_gpu\": 16\n",
    "}\n",
    "```\n",
    "\n",
    "  * `\"device\": \"nvme\"`: 데이터를 CPU RAM이 아닌 NVMe SSD에 저장합니다.\n",
    "  * `\"nvme_path\"`: 데이터를 저장할 NVMe 마운트 경로를 지정해야 합니다.\n",
    "\n",
    "\n",
    "* **ZeRO-Offload (CPU):** VRAM의 한계를 **CPU RAM**으로 확장합니다. GPU가 계산하는 동안 CPU는 데이터 전송(PCIe)을 수행하여 병목을 숨깁니다.\n",
    "* **ZeRO-Infinity (NVMe):** CPU RAM의 한계마저 **NVMe SSD**로 확장합니다. 1조(Trillion) 파라미터가 넘는, 사실상 '무한한' 크기의 모델을 훈련할 수 있게 해주는 최종 기술입니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39fd4af",
   "metadata": {},
   "source": [
    "## +) Data Efficiency\n",
    "\n",
    "https://www.deepspeed.ai/docs/config-json/#data-efficiency\n",
    "\n",
    "보통 GPU는 연산을 무진장 많이 하니까 CPU -> GPU 로 데이터 로딩이 병목현상이 되는 경우가 많습니다. 이걸 그냥 기다리고 있으면 GPU가 놀게 되니까, DeepSpeed에서는 데이터 로딩을 더 효율적으로 하기 위한 몇가지 옵션들을 제공합니다. json 에서 설정을 해줄 수 있습니다. 자세한건 저도 잘 모르고 매번 업데이트 되어서 구현된 기능이 변경되곤 합니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dist",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
